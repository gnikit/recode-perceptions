
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.0, mkdocs-material-8.3.9">
    
    
      
        <title>Training a Convolutional Neural Network in PyTorch - ReCoDE Perceptions</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/main.1d29e8d0.min.css">
      
        
        <link rel="stylesheet" href="assets/stylesheets/palette.cbb835fc.min.css">
        
      
      
    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL(".",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="deep-blue" data-md-color-accent="deep-blue">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#training-a-convolutional-neural-network-in-pytorch" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="index.html" title="ReCoDE Perceptions" class="md-header__button md-logo" aria-label="ReCoDE Perceptions" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            ReCoDE Perceptions
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Training a Convolutional Neural Network in PyTorch
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="deep-blue" data-md-color-accent="deep-blue"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
            </label>
          
        
          
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="deep-orange" data-md-color-accent="deep-orange"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/ImperialCollegeLondon/recode-perceptions" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="index.html" title="ReCoDE Perceptions" class="md-nav__button md-logo" aria-label="ReCoDE Perceptions" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    ReCoDE Perceptions
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/ImperialCollegeLondon/recode-perceptions" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="index.html" class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="1-cnn-intro.html" class="md-nav__link">
        Understanding Convolutional Neural Networks in PyTorch
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Training a Convolutional Neural Network in PyTorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="2-cnn-training.html" class="md-nav__link md-nav__link--active">
        Training a Convolutional Neural Network in PyTorch
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#training-locally" class="md-nav__link">
    Training Locally
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hyperparameter-optimisation" class="md-nav__link">
    Hyperparameter Optimisation
  </a>
  
    <nav class="md-nav" aria-label="Hyperparameter Optimisation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#hyperparameter-optimisation-using-wandbai" class="md-nav__link">
    Hyperparameter optimisation using wandb.ai
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#export-to-hpc" class="md-nav__link">
    Export to HPC
  </a>
  
    <nav class="md-nav" aria-label="Export to HPC">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#getting-started" class="md-nav__link">
    Getting Started
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dataset" class="md-nav__link">
    Dataset
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setting-up-the-environment" class="md-nav__link">
    Setting up the Environment
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#submitting-jobs-to-the-hpc" class="md-nav__link">
    Submitting jobs to the HPC
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#training-locally" class="md-nav__link">
    Training Locally
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hyperparameter-optimisation" class="md-nav__link">
    Hyperparameter Optimisation
  </a>
  
    <nav class="md-nav" aria-label="Hyperparameter Optimisation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#hyperparameter-optimisation-using-wandbai" class="md-nav__link">
    Hyperparameter optimisation using wandb.ai
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#export-to-hpc" class="md-nav__link">
    Export to HPC
  </a>
  
    <nav class="md-nav" aria-label="Export to HPC">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#getting-started" class="md-nav__link">
    Getting Started
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dataset" class="md-nav__link">
    Dataset
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setting-up-the-environment" class="md-nav__link">
    Setting up the Environment
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#submitting-jobs-to-the-hpc" class="md-nav__link">
    Submitting jobs to the HPC
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
  <a href="https://github.com/ImperialCollegeLondon/recode-perceptions/tree/main/docs/2-cnn-training.md" title="Edit this page" class="md-content__button md-icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"/></svg>
  </a>


  



<h1 id="training-a-convolutional-neural-network-in-pytorch">Training a Convolutional Neural Network in PyTorch</h1>
<p>In this exercise we will fine-tune a pretrained CNN to learn to predict perception scores. Before we dive into the components of the model, we will just get the code up and running with the default settings.</p>
<h2 id="training-locally">Training Locally</h2>
<p>Locally means using the current hardware in your computer or laptop to run model training. If you have not yet done so, now is a good time to clone this repository into your local drive (see Getting Started).</p>
<p>Let's check the model training runs locally (albeit slowly without a GPU). From recode-perceptions run:</p>
<div class="highlight"><pre><span></span><code>python3 -m deep_cnn                     <span class="se">\</span>
--epochs<span class="o">=</span><span class="m">1</span>                              <span class="se">\</span>
--data_dir<span class="o">=</span>tests/places_test_input      <span class="se">\</span>
--wandb<span class="o">=</span>False                           <span class="se">\</span>
</code></pre></div>
<p>A brief description of all arguments is returned with the following:</p>
<div class="highlight"><pre><span></span><code>python3 -m deep_cnn -h
usage: __main__.py <span class="o">[</span>-h<span class="o">]</span> <span class="o">[</span>--epochs EPOCHS<span class="o">]</span> <span class="o">[</span>--batch_size BATCH_SIZE<span class="o">]</span>
                   <span class="o">[</span>--model MODEL<span class="o">]</span> <span class="o">[</span>--pre PRE<span class="o">]</span> <span class="o">[</span>--root_dir ROOT_DIR<span class="o">]</span> <span class="o">[</span>--lr LR<span class="o">]</span>
                   <span class="o">[</span>--run_name RUN_NAME<span class="o">]</span> <span class="o">[</span>--data_dir DATA_DIR<span class="o">]</span> <span class="o">[</span>--wandb WANDB<span class="o">]</span>

optional arguments:
  -h, --help            show this <span class="nb">help</span> message and <span class="nb">exit</span>
  --epochs EPOCHS       number of training epochs
  --batch_size BATCH_SIZE
                        batch size <span class="k">for</span> SGD
  --model MODEL         pre-trained model
  --pre PRE             pre-processing <span class="k">for</span> image input
  --root_dir ROOT_DIR   path to recode-perceptions
  --lr LR               learning rate
  --run_name RUN_NAME   unique name to identify hyperparameter choices
  --data_dir DATA_DIR   path to input data
  --wandb WANDB         track progress <span class="k">in</span> wandb.ai
</code></pre></div>
<p>where <code>root_dir</code> is your local path to recode-perceptions, and <code>data_dir</code> is your path to <code>input/places365standard_easyformat/places365_standard</code>. If you have not yet downloaded the full image dataset, but you want to run this script locally, you can run the test images, tests/test_input/test_images/.</p>
<p>You should see the following output in your terminal:</p>
<div class="highlight"><pre><span></span><code>Running on cuda device
Epoch 0:   2%|██▎                                 | 35/1697 [00:19&lt;15:28,  1.79batch/s, loss=3.06]
</code></pre></div>
<p>If you have a GPU locally, you will also see 'Running on <code>cuda</code> device'. However, this will be replaced by CPU if no GPU device is found. The model is training through its first epoch batch by batch. This one epoch is expected to take 15 minutes to complete.</p>
<p>Once finished, the full log can be found in the folder outputs/logger/. You will find an annotated version in the repository.</p>
<h2 id="hyperparameter-optimisation">Hyperparameter Optimisation</h2>
<p>Let's take a look at the design choices which can be made for model training.</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Description</th>
<th>Trade Offs</th>
<th>References</th>
</tr>
</thead>
<tbody>
<tr>
<td>Epochs</td>
<td>Determine the number of times to run through the entire training batch.</td>
<td>Typically there is an inflection point where decreases in loss are marginal. Continued increase after this reflects overfitting</td>
<td><a href="https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote12.html">Bias-Variance Trade-Off</a></td>
</tr>
<tr>
<td>Batch size</td>
<td>Number of samples for each training step</td>
<td>Smaller batch sizes increase performance of stochastic gradient descent algorithms while also preserving memory by loading in batches. Larger batch size can speed up computation.</td>
<td><a href="https://stats.stackexchange.com/questions/164876/what-is-the-trade-off-between-batch-size-and-number-of-iterations-to-train-a-neu">Batch-size</a></td>
</tr>
<tr>
<td>Learning Rate</td>
<td>Step size of parameter update at each iteration</td>
<td>A larger learning rate requires fewer epochs but can easily result in unstable results such as local minima. Smaller learning rates can fail to find an optima at all. Adaptive learning rates consider large step sizes in the earlier epochs of training and reduces the step size in later epochs</td>
<td><a href="https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/">Learning Rates</a></td>
</tr>
<tr>
<td>Model</td>
<td>Pre-trained model</td>
<td>Can vary by size, depth, structure and trainable parameters Smaller models are faster to train while deeper model typically achieve higher levels of abstraction. Models with dropout can avoid overfitting,</td>
<td><a href="https://pytorch.org/vision/stable/models.html">PyTorch pre-trained models</a></td>
</tr>
<tr>
<td>Pre-processing</td>
<td>Image pre-processing required for model input</td>
<td>Pre-trained models have parameters trained within a given range and perform better when the target dataset distribution is closer matched to the source dataset distribution</td>
<td>-</td>
</tr>
</tbody>
</table>
<p>There is never a one-rule-fits-all-approach to training a deep neural network. Design choices can be guided by the domain task, the dataset size and distribution, hardware constraints, time constraints or all of the above. The wonder of moden software and computing is that the cycle of iterating on model choices has been sped up enormously:</p>
<p><img alt="alt text" src="images/image_tasks.jpeg" title="Hyperparameter Optimisation Cycle" /></p>
<p>There exists many types of <a href="https://en.wikipedia.org/wiki/Hyperparameter_optimization">search algorithms</a> for finding the optimal hyperparameters, such as gridded search, random search and Bayesian optimisation. We would like to add a priori the wisdom of the crowd. There has been a lot written about deep learning hyperparameters, so we don't need to go in blind. To help you configure some intervals, consider the following questions</p>
<ol>
<li>How many epochs will ensure you have reached the global minima?</li>
<li>How much memory constraints do you have on model and batch size?</li>
<li>What ratio of batch size to data samples/classes is considered a benchmark?</li>
<li>What is a typical learning rate for similar image classification tasks?</li>
<li>How can an adaptive learning rate be implemented?</li>
<li>Which model has shown the best performance on benchmark datasets? Is there a PyTorch version to download pre-trained weights?</li>
<li>Does this model require certain pre-processing? If yes, you will have to add the preprocessing to dataset_generator.py</li>
</ol>
<p>Once you have answered the questions above, you will have a constraint on what is reasonable to test.</p>
<p>When performing hyperparameter optimisation, we will not evaluate our test performance during training. This test set will be a hold-out set used only to evaluate the performance of our final model after training. Instead, we evaluate the performance of the model during training on the validation set. This is done typically to avoid overfitting to the test set during hyperparameter optimisation. Our goal in training is to test the generalisability of our training paradigm and the test-set allows us to do this.</p>
<h3 id="hyperparameter-optimisation-using-wandbai">Hyperparameter optimisation using <code>wandb.ai</code></h3>
<p>We will use weights and biases to track our model training and validation. This platform uses a lightweight python package to log metrics during training. To use this, you will need to create an account at <a href="https://wandb.ai/site">https://wandb.ai/site</a>. You can create an account using your GitHub or Gmail. You will be prompted to create a username, keep a note of this. Once you have completed your registration, you will be directed to a landing page with an API key, keep a note of this too.</p>
<p>Once you have configured your user credentials, create a new project called recode-perceptions. This folder will log all of our training runs. In order for python to get access to your personal user credentials, you will have to set them as environmental variables which python then accesses using <code>os.getenv("WB_KEY")</code>. Set your environmental variables as follows:</p>
<div class="highlight"><pre><span></span><code><span class="nb">export</span> <span class="nv">WB_KEY</span><span class="o">=</span>API_KEY
<span class="nb">export</span> <span class="nv">WB_PROJECT</span><span class="o">=</span><span class="s2">&quot;recode-perceptions&quot;</span>
<span class="nb">export</span> <span class="nv">WB_USER</span><span class="o">=</span><span class="s2">&quot;username&quot;</span>
</code></pre></div>
<p>If you now run the scripts with <code>--wandb=True</code>, you should begin to see the metrics being tracked on the platform:</p>
<p><img alt="alt text" src="images/wandb.png" title="Logging metrics using wandb" /></p>
<h2 id="export-to-hpc">Export to HPC</h2>
<h3 id="getting-started">Getting Started</h3>
<p>We can move our training to the HPC to utilise the hardware resources available to us on the university cluster. If you haven't yet had the chance, now is a good time to read more about the computer services offered by the <a href="https://www.imperial.ac.uk/computational-methods/hpc/">High Performance Computing</a> cluster. There is a lot of help available from online resources, support clinics, or training courses.</p>
<p>You can <a href="https://www.imperial.ac.uk/admin-services/ict/self-service/research-support/rcs/support/getting-started/using-ssh/">connect to the HPC using SSH</a>:</p>
<div class="highlight"><pre><span></span><code>ssh -XY user@login.hpc.ic.ac.uk
</code></pre></div>
<p>The <a href="https://www.imperial.ac.uk/admin-services/ict/self-service/research-support/rcs/support/getting-started/data-management/">Research Data Store is accessible</a> as a network share, at the location <code>\\rds.imperial.ac.uk\RDS\user\</code>. There you will have read/write access to your home and ephemeral spaces, along with any projects you are a member of.</p>
<p>You will be prompted to enter your password. You now have terminal access to your remote HPC resources. You will need a remote copy of the recode-perceptions repository. Git clone this directly as in the main repo README.md and proceed to download the Image dataset</p>
<h3 id="dataset">Dataset</h3>
<p>The dataset can be downloaded by executing the data_download.sh script.</p>
<p><code>GLOBIGNORE</code> specifies folders which should be ignored when performing recursive deletes.</p>
<p>The datasets were not released by us, and we do not claim any rights on them. Use the datasets at your responsibility and make sure you fulfil the licenses that they were released with. If you use any of the datasets please consider citing the original authors of <a href="http://places2.csail.mit.edu/PAMI_places.pdf">Places365</a>.</p>
<h3 id="setting-up-the-environment">Setting up the Environment</h3>
<p>Similarly to how we created a virtual environment locally, we will have to configure our environment on the HPC. We will be using <a href="https://anaconda.org/">anaconda</a> this time, which is a distribution of Python which aims to simplify package management.</p>
<p>The environment.sh bash file is in the main repo and can be executed with the following:</p>
<div class="highlight"><pre><span></span><code>sh environment.sh
</code></pre></div>
<p>This file first loads the <code>conda</code> and <code>cuda</code> module and then initiates a Python 3.7 environment before configuring the necessary packages. This should take a few minutes.</p>
<h3 id="submitting-jobs-to-the-hpc">Submitting jobs to the HPC</h3>
<p>Once the environment is configured, we can submit the programme requesting access to HPC GPU's. First, copy over your <code>wandb.ai</code> credentials into the <code>submit.pbs</code> script (lines 10-12). Then run:</p>
<div class="highlight"><pre><span></span><code>qsub submit.pbs
</code></pre></div>
<p>See HPC resources for <a href="https://www.imperial.ac.uk/admin-services/ict/self-service/research-support/rcs/computing/job-sizing-guidance/gpu/">how to request GPUs</a>. Job submissions will result in the creation of two files, stderr and stdout. Inspecting these files you will see a similar output to when we ran the programme locally. In addition, logger will write file to <code>outputs/logger</code>, as well as training being logged on <code>wandb.ai</code>.</p>
<p>Revisit Hyperparameter Optimisation and iterate on model choices using run_name to track design choices.</p>

              
            </article>
            
          </div>
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" hidden>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
            Back to top
          </a>
        
      </main>
      
        <footer class="md-footer">
  
    
    <nav class="md-footer__inner md-grid" aria-label="Footer" >
      
        
        <a href="1-cnn-intro.html" class="md-footer__link md-footer__link--prev" aria-label="Previous: Understanding Convolutional Neural Networks in PyTorch" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Understanding Convolutional Neural Networks in PyTorch
            </div>
          </div>
        </a>
      
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": ".", "features": ["navigation.instant", "navigation.top"], "search": "assets/javascripts/workers/search.b97dbffb.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="assets/javascripts/bundle.6c7ad80a.min.js"></script>
      
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
      
    
  </body>
</html>